{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements of the paper implemented so far:\n",
    "* regularized loss\n",
    "* generalized formalism for split score / optimal leaf weight calculation\n",
    "* default direction for missing values (also available for other algorithms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not yet implemented because of general scariness:\n",
    "* the _weighted quantile sketch_ histogramming strategy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Chen et al. 2016, XGBoost: A Scalable Tree Boosting System](https://dl.acm.org/doi/10.1145/2939672.2939785)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to formalism"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{loss}^{(t)} = \\sum_{i=1}^n l(\\text{target}_i, \\text{cumulative estimate}^{(t-1)}_i + \\text{change in estimate}^{(t)}_i ) + \\Omega\\left(\\text{new estimation function}^{(t)}\\right)$$\n",
    "\n",
    "$t$ is the boosting iteration / tree number and $i$ is a single observation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use\n",
    "* $ \\text{new estimation function}^{(t)} = f_t $\n",
    "* $ \\text{change in estimate}^{(t)}_i = f_t(x_i)$\n",
    "* $ \\text{target}_i = y_i$\n",
    "* $ \\text{cumulative estimate}^{(t-1)}_i = \\hat{y}^{(t-1)}_i$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can shorten the $\\text{loss} ^ {(t)}$ description to a more cryptic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{loss} ^ {(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}^{(t-1)}_i + f^{(t)}(x_i) ) + \\Omega \\left( f^{(t)} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the regularization the authors use\n",
    "$$ \\Omega (f_t) = \\gamma N^{(t)}_\\text{leafs} + \\frac{1}{2} \\lambda \\sum^{N^{(t)}_\\text{leafs}}_j w_j^2 $$\n",
    "\n",
    "where $\\gamma$ is some constant and $w_j$ is a leaf weight (seems like the $\\gamma_{jm}$ from Friedman et al. but isn't clarified)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taylor series approximation ftw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating how $l(y_i, \\hat{y}^{(t-1)}_i + f^{(t)}(x_i) )$ changes for varying $f$, with a 2nd order [Taylor series](https://en.wikipedia.org/wiki/Taylor_series), the authors write\n",
    "$$ \\text{loss}^{(t)} \\approx \\sum_{i=1}^n l(y_i, \\hat{y}^{(t-1)}_i)  + g_i f^{(t)}(x_i) + \\frac{1}{2} h_i \\left(f^{(t)}(x_i)\\right)^2   + \\Omega\\left(f^{(t)}\\right) $$\n",
    "\n",
    "where $g_i = \\frac{\\partial l(y_i, \\hat{y}^{(t-1)}_i)}{\\partial \\hat{y}^{(t-1)}}$ and $h_i = \\frac{\\partial^2 l(y_i, \\hat{y}^{(t-1)}_i)}{\\partial \\left(\\hat{y}^{(t-1)}\\right)^2}$ contain the 1st and 2nd partial derivatives of the loss function $l$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key property of a tree is that every observation $x_i$ is part of exactly one leaf $I_j$, and hence exactly one predicted value / leaf weight $w_j$, or in cryptic\n",
    "$$ f^{(t)}(x_i) = w_j \\space \\forall x_i \\in I_j$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal leaf weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this aspect and the loss above the authors derive the optimal leaf values $w_j^*$ as \n",
    "\n",
    "$$ w_j^* = - \\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda} $$\n",
    "\n",
    "where $I_j$ is the set of observations / **I**nstances in leaf $j$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the loss for the new tree (ignoring the cumulative part $l(y_i, \\hat{y}^{(t-1)}_i)$ which is not influenced by the optimization of $\\text{loss}^{(t)}$) is dervied as\n",
    "\n",
    "$$ \\text{loss new tree}^{(t)} \\approx - \\frac{1}{2} \\sum_{j=1}^{N_\\text{leafs}} \\frac{\\left( \\sum_{i \\in I_j} g_i \\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma N_\\text{leafs} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we contemplate to introduce another split ($N_\\text{leafs, new} = N_\\text{leafs} + 1$) we have the difference of $\\text{loss new tree with another split}^{(t)} - \\text{loss new tree}^{(t)}$\n",
    "\n",
    "$$  = \\left(- \\frac{1}{2} \\sum_{j=1}^{N_\\text{leafs}+1} \\frac{\\left( \\sum_{i \\in I_{\\text{new},j}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{new},j}} h_i + \\lambda} + \\gamma \\left(N_\\text{leafs} + 1\\right) \\right) - \\left( - \\frac{1}{2} \\sum_{j=1}^{N_\\text{leafs}} \\frac{\\left( \\sum_{i \\in I_j} g_i \\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma N_\\text{leafs} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  = - \\frac{1}{2} \\left( \\frac{\\left( \\sum_{i \\in I_{\\text{left}}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{left}}} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_{\\text{right}}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{right}}} h_i + \\lambda} \\right) + \\gamma + \\frac{1}{2} \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  = - \\frac{1}{2} \\left( \\frac{\\left( \\sum_{i \\in I_{\\text{left}}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{left}}} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_{\\text{right}}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{right}}} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right) + \\gamma   $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three components in the round braces above is then what can be used to decide on whether to split the set of observations $I$ into $I_\\text{left}$ and $I_\\text{right}$ or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example derivatives for regression and binary classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at example $g$ (1st order derivative) and $h$ (2nd order derivative) values. \n",
    "\n",
    "#### Regression - Least squares \n",
    "\n",
    "$$l(y_i, \\hat{y}^{(t-1)}_i) = \\frac{\\left(y_i - \\hat{y}^{(t-1)}_i\\right)^2}{2}$$\n",
    "\n",
    "\n",
    "* $g_i = \\hat{y}^{(t-1)}_i - y_i$\n",
    "* $h_i = 1$\n",
    "\n",
    "#### Binary classification - negative binomial log-likelihood \n",
    "\n",
    "$$l(y_i, \\hat{y}^{(t-1)}_i) = \\log\\left(1 + \\exp \\left(-2 y_i \\cdot \\hat{y}^{(t-1)}_i\\right)\\right)$$\n",
    "\n",
    "where $y_i \\in \\{-1,1\\}$ and $\\hat{y}^{(t-1)}_i \\in \\mathbb{R}$\n",
    "\n",
    "* $g_i = - \\frac{2 y_i}{(\\exp(2 y_i \\hat{y}^{(t-1)}_i) + 1)}$\n",
    "* $h_i = \\frac{4 y_i^2 \\exp(2 y_i \\hat{y}^{(t-1)}_i))}{(\\exp(2 y_i \\hat{y}^{(t-1)}_i) + 1)^2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default direction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a worked example see [this blog post](https://medium.com/hypatai/how-xgboost-handles-sparsities-arising-from-of-missing-data-with-an-example-90ce8e4ba9ca). \n",
    "\n",
    "The basic idea of Algorithm 3 for the \"default direction\" in the XGBoost paper is for each split search along on feature:\n",
    "* if list of features contains missing values:\n",
    "    1. pretend missing features have values **below** the threshold and compute the loss / split score\n",
    "    2. pretend missing features have values **above** the threshold and compute the loss / split score\n",
    "    3. compare the two scores and use the better one to define the \"default direction\" for splits on missing values\n",
    "* else: compute loss / split score without \"default direction\" garnish\n",
    "\n",
    "Note: this bit can be used for decision trees independently of the rest of the paper / choice of loss / boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as sk_datasets\n",
    "from scipy import stats\n",
    "\n",
    "from random_tree_models.models.decisiontree.visualize import show_tree\n",
    "import random_tree_models.models.xgboost as xgboost\n",
    "from random_tree_models.params import MetricNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_missing(\n",
    "    X: np.ndarray,\n",
    "    p_missing: T.List[float],\n",
    "    missing_value=np.nan,\n",
    "    rng: np.random.RandomState | None = None,\n",
    ") -> np.ndarray:\n",
    "    X_missing = X.copy()\n",
    "    if X.shape[1] != len(p_missing):\n",
    "        raise ValueError(f\"{X.shape[1]=} != {len(p_missing)=}\")\n",
    "\n",
    "    n = len(X)\n",
    "    for i, p in enumerate(p_missing):\n",
    "        mask = stats.bernoulli.rvs(p=p, size=n, random_state=rng)\n",
    "        X_missing[:, i] = np.where(mask, missing_value, X_missing[:, i])\n",
    "\n",
    "    return X_missing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sk_datasets.make_classification(\n",
    "    n_samples=1_000,\n",
    "    n_features=2,\n",
    "    n_classes=2,\n",
    "    n_redundant=0,\n",
    "    class_sep=2,\n",
    "    random_state=rng,\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBoostClassifier(\n",
    "    measure_name=MetricNames.xgboost, max_depth=2, n_trees=3, lam=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(model.trees_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X)\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X_plot)[:, 1]\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_prob.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coefs = sk_datasets.make_regression(\n",
    "    n_samples=1_000, n_features=2, n_targets=1, coef=True, random_state=rng\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=y, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBoostRegressor(\n",
    "    measure_name=MetricNames.xgboost, max_depth=2, n_trees=3, lam=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(model.trees_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_plot)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "ax = axs[0]\n",
    "sns.scatterplot(x=X_plot[:, 0], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "\n",
    "ax = axs[1]\n",
    "sns.scatterplot(x=X_plot[:, 1], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_pred.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "ax = axs[0]\n",
    "sns.scatterplot(x=X[:, 0], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "sns.scatterplot(x=X[:, 0], y=y, ax=ax, alpha=0.1, label=\"actual\")\n",
    "\n",
    "ax = axs[1]\n",
    "sns.scatterplot(x=X[:, 1], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "sns.scatterplot(x=X[:, 1], y=y, ax=ax, alpha=0.1, label=\"actual\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with missing feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sk_datasets.make_classification(\n",
    "    n_samples=1_000,\n",
    "    n_features=2,\n",
    "    n_classes=2,\n",
    "    n_redundant=0,\n",
    "    class_sep=2,\n",
    "    random_state=rng,\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_missing = [0.0, 0.5]\n",
    "X_missing = make_missing(X, p_missing)\n",
    "np.isnan(X_missing).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBoostClassifier(\n",
    "    measure_name=MetricNames.xgboost,\n",
    "    max_depth=2,\n",
    "    n_trees=3,\n",
    "    lam=0.0,\n",
    "    ensure_all_finite=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_missing, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(model.trees_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X)\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X_plot)[:, 1]\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_prob.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with missing feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coefs = sk_datasets.make_regression(\n",
    "    n_samples=1_000, n_features=2, n_targets=1, coef=True, random_state=rng\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=y, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_missing = [0.0, 0.5]\n",
    "X_missing = make_missing(X, p_missing)\n",
    "np.isnan(X_missing).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBoostRegressor(\n",
    "    measure_name=MetricNames.xgboost,\n",
    "    max_depth=2,\n",
    "    n_trees=3,\n",
    "    lam=0.0,\n",
    "    ensure_all_finite=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_missing, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(model.trees_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_plot)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "ax = axs[0]\n",
    "sns.scatterplot(x=X_plot[:, 0], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "\n",
    "ax = axs[1]\n",
    "sns.scatterplot(x=X_plot[:, 1], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_pred.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "ax = axs[0]\n",
    "sns.scatterplot(x=X[:, 0], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "sns.scatterplot(x=X[:, 0], y=y, ax=ax, alpha=0.1, label=\"actual\")\n",
    "\n",
    "ax = axs[1]\n",
    "sns.scatterplot(x=X[:, 1], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "sns.scatterplot(x=X[:, 1], y=y, ax=ax, alpha=0.1, label=\"actual\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
