{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosted trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Friedman 2001, Greedy Function Approximation: A Gradient Boosting Machine](https://www.jstor.org/stable/2699986)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The core algorithm\n",
    "\n",
    "### Training\n",
    "\n",
    "1. select a `loss(y_true,y_pred)`, e.g. least squares for regression\n",
    "2. make constant baseline estimate of `y_pred`, e.g. average of `y_true`\n",
    "3. iterate between\n",
    "    * calculate gap `gap(y_pred,y_true)` to obtain new `y_true` only containing bits we got wrong so far and\n",
    "    * fit model (tree) to predict new `y_true` using loss-optimal leaf weights\n",
    "    * store model\n",
    "    * stop once `y_true` is pretty much all zero\n",
    "\n",
    "\n",
    "So at the end you have a baseline estimate and a bunch of models / boosts correcting the prediction for each observation. \n",
    "\n",
    "For a more formal version see the [_Algorithm 1 Gradient_Boost_ in Friedman et al. 2001](https://www.jstor.org/stable/2699986).\n",
    "\n",
    "In `nbs/xgboost.ipynb` the math is spelled out in a bit more detail, using the notation of the paper [_XGBoost: A Scalable Tree Boosting System_ by Chen et al. 2016](http://arxiv.org/abs/1603.02754).\n",
    "\n",
    "### Inference\n",
    "\n",
    "1. collect constant baseline estimate\n",
    "2. compute corrections with each tree / boost\n",
    "3. sum baseline estimate and model predictions\n",
    "4. (transform above sum, e.g. for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as sk_datasets\n",
    "\n",
    "from random_tree_models.decisiontree.visualize import show_tree\n",
    "import random_tree_models.gradientboostedtrees as gbtree\n",
    "from random_tree_models.scoring import MetricNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brace your self. We are looking at Algorithm 5 (LK_TreeBoost) in Friedman 2001, Greedy Function Approximation: A Gradient Boosting Machine.\n",
    "\n",
    "To use boosting for classification Friedman et al. map a binary target `y` to -1 and 1 and the negative binomial log-likelihood as a loss, inducing a freaky `dy` in a continuous space that is not bounded by -1 and 1 or 0 and 1.\n",
    "\n",
    "The baseline estimate is: $0.5 \\log \\frac{P(y=1)}{P(y=-1)} = 0.5 \\log \\frac{\\text{mean}(y==1)}{\\text{mean}(y==-1)}$\n",
    "\n",
    "The used negative binomial log-likelihood loss is: $\\text{loss} = \\log\\left(1+\\text{exp}(-2 \\cdot y \\cdot \\text{estimate})\\right)$\n",
    "\n",
    "Hence the loss changes with the estimate of each observation by: $\\frac{d\\text{loss}}{d\\text{estimate}} = dy = \\frac{2 \\cdot y}{1 + \\exp(2 \\cdot y \\cdot \\text{estimate})}$\n",
    "\n",
    "To clarify: $y$ is -1 or 1. (baseline) $\\text{estimate}$ is something between $-\\infty$ and $\\infty$, as is $dy$.\n",
    "\n",
    "This `dy` is what a model is trying to predict and what gets updated for the next model. So since our models here are regression decision trees each leaf contains an update to `dy`.\n",
    "\n",
    "To compute the final estimate add our baseline estimate and all the leaf values for $n$ models we have trained: $\\text{estimate} = \\text{baseline estimate} + dy_0 + dy_1 + ... dy_n$\n",
    "\n",
    "Then we have to map back to the space of probabilities (0 to 1) for this to be useful, using: $P(y=1) = \\frac{1}{1 + \\exp(-2 \\cdot \\text{estimate})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sk_datasets.make_classification(\n",
    "    n_samples=1_000,\n",
    "    n_features=2,\n",
    "    n_classes=2,\n",
    "    n_redundant=0,\n",
    "    class_sep=2,\n",
    "    random_state=rng,\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gbtree.GradientBoostedTreesClassifier(\n",
    "    measure_name=MetricNames.friedman_binary_classification, max_depth=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(model.trees_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X)\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X_plot)[:, 1]\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_prob.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "Algorithm 2 (LS_Boost) is used here. Since it is pretty much what is stated in _The core algorithm_ above, an explanation is omitted here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coefs = sk_datasets.make_regression(\n",
    "    n_samples=1_000, n_features=2, n_targets=1, coef=True, random_state=rng\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=y, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gbtree.GradientBoostedTreesRegressor(\n",
    "    measure_name=MetricNames.variance, max_depth=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(model.trees_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_plot)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "ax = axs[0]\n",
    "sns.scatterplot(x=X_plot[:, 0], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "\n",
    "ax = axs[1]\n",
    "sns.scatterplot(x=X_plot[:, 1], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_pred.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "ax = axs[0]\n",
    "sns.scatterplot(x=X[:, 0], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "sns.scatterplot(x=X[:, 0], y=y, ax=ax, alpha=0.1, label=\"actual\")\n",
    "\n",
    "ax = axs[1]\n",
    "sns.scatterplot(x=X[:, 1], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "sns.scatterplot(x=X[:, 1], y=y, ax=ax, alpha=0.1, label=\"actual\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
