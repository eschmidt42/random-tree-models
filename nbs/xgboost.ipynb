{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Chen et al. 2016, XGBoost: A Scalable Tree Boosting System](https://dl.acm.org/doi/10.1145/2939672.2939785)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The core algorithm\n",
    "\n",
    "### Training\n",
    "\n",
    "1. make a simple baseline estimate, e.g. average of `y`\n",
    "2. compute the differences of `y` and the baseline -> `dy`\n",
    "3. while `dy` values are too large:\n",
    "    * train new model predicting `dy`\n",
    "    * update remaining `dy`\n",
    "\n",
    "So at the end you have a baseline estimate and a bunch of models correcting the prediction for each observation. \n",
    "\n",
    "For the actual algorithms one needs to define a loss to be minimized to know what `dy` exactly, see the _Algorithm 1 Gradient_Boost_ in Friedman et al. 2001.\n",
    "\n",
    "### Inference\n",
    "\n",
    "1. for each model compute its prediction\n",
    "2. sum baseline estimate and model predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{loss}^{(t)} = \\sum_{i=1}^n l(\\text{target}_i, \\text{cumulative estimate}^{(t-1)}_i + \\text{change in estimate}^{(t)}_i ) + \\Omega\\left(\\text{new estimation function}^{(t)}\\right)$$\n",
    "\n",
    "$t$ is the boosting iteration / tree number and $i$ is a single observation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use\n",
    "* $ \\text{new estimation function}^{(t)} = f_t $\n",
    "* $ \\text{change in estimate}^{(t)}_i = f_t(x_i)$\n",
    "* $ \\text{target}_i = y_i$\n",
    "* $ \\text{cumulative estimate}^{(t-1)}_i = \\hat{y}^{(t-1)}_i$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can shorten the $\\text{loss} ^ {(t)}$ description to a more cryptic\n",
    "$$ \\text{loss}^{(t)} = \\sum_{i=1}^n l(y_i, \\hat{y}^{(t-1)}_i + f^{(t)}(x_i) ) + \\Omega\\left(f^{(t)}\\right) $$ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the regularization the authors use\n",
    "$$ \\Omega (f_t) = \\gamma N^{(t)}_\\text{leafs} + \\frac{1}{2} \\lambda \\sum^{N^{(t)}_\\text{leafs}}_j w_j^2$$\n",
    "\n",
    "where $\\gamma$ is some constant and $w_j$ is a leaf weight (seems like the $\\gamma_{jm}$ from Friedman et al. but isn't clarified)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating how $l(y_i, \\hat{y}^{(t-1)}_i + f^{(t)}(x_i) )$ changes for varying $f$, with a 2nd order Taylor polynomial, the authors write\n",
    "$$ \\text{loss}^{(t)} \\approx \\sum_{i=1}^n l(y_i, \\hat{y}^{(t-1)}_i)  + g_i f^{(t)}(x_i) + \\frac{1}{2} h_i \\left(f^{(t)}(x_i)\\right)^2   + \\Omega\\left(f^{(t)}\\right) $$\n",
    "\n",
    "where $g_i = \\frac{\\partial l(y_i, \\hat{y}^{(t-1)}_i)}{\\partial \\hat{y}^{(t-1)}}$ and $h_i = \\frac{\\partial^2 l(y_i, \\hat{y}^{(t-1)}_i)}{\\partial \\left(\\hat{y}^{(t-1)}\\right)^2}$ contain the 1st and 2nd partial derivatives of the loss function $l$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key property of a tree is that every observation $x_i$ is part of exactly one leaf $I_j$, and hence exactly one predicted value / leaf weight $w_j$, or in cryptic\n",
    "$$ f^{(t)}(x_i) = w_j \\space \\forall x_i \\in I_j$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this aspect and the loss above the authors derive the optimal leaf values $w_j^*$ as \n",
    "\n",
    "$$ w_j^* = - \\frac{\\sum_{i \\in I_j} g_i}{\\sum_{i \\in I_j} h_i + \\lambda} $$\n",
    "\n",
    "where $I_j$ is the set of observations / **I**nstances in leaf $j$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the loss for the new tree (ignoring the cumulative part $l(y_i, \\hat{y}^{(t-1)}_i)$ which is not influenced by the optimization of $\\text{loss}^{(t)}$) is dervied as\n",
    "\n",
    "$$ \\text{loss new tree}^{(t)} \\approx - \\frac{1}{2} \\sum_{j=1}^{N_\\text{leafs}} \\frac{\\left( \\sum_{i \\in I_j} g_i \\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma N_\\text{leafs} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we contemplate to introduce another split ($N_\\text{leafs, new} = N_\\text{leafs} + 1$) we have the difference of $\\text{loss new tree with another split}^{(t)} - \\text{loss new tree}^{(t)}$\n",
    "\n",
    "$$  = \\left(- \\frac{1}{2} \\sum_{j=1}^{N_\\text{leafs}+1} \\frac{\\left( \\sum_{i \\in I_{\\text{new},j}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{new},j}} h_i + \\lambda} + \\gamma \\left(N_\\text{leafs} + 1\\right) \\right) - \\left( - \\frac{1}{2} \\sum_{j=1}^{N_\\text{leafs}} \\frac{\\left( \\sum_{i \\in I_j} g_i \\right)^2}{\\sum_{i \\in I_j} h_i + \\lambda} + \\gamma N_\\text{leafs} \\right) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  = - \\frac{1}{2} \\left( \\frac{\\left( \\sum_{i \\in I_{\\text{left}}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{left}}} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_{\\text{right}}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{right}}} h_i + \\lambda} \\right) + \\gamma + \\frac{1}{2} \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$  = - \\frac{1}{2} \\left( \\frac{\\left( \\sum_{i \\in I_{\\text{left}}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{left}}} h_i + \\lambda} + \\frac{\\left( \\sum_{i \\in I_{\\text{right}}} g_i \\right)^2}{\\sum_{i \\in I_{\\text{right}}} h_i + \\lambda} - \\frac{\\left( \\sum_{i \\in I} g_i \\right)^2}{\\sum_{i \\in I} h_i + \\lambda} \\right) + \\gamma   $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three components in the round braces above is then what can be used to decide on whether to split the set of observations $I$ into $I_\\text{left}$ and $I_\\text{right}$ or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at example $g$ and $h$ values. \n",
    "\n",
    "Regression - Least squares $l(y_i, \\hat{y}^{(t-1)}_i) = \\frac{\\left(y_i - \\hat{y}^{(t-1)}_i\\right)^2}{2}$:\n",
    "* $g_i = \\hat{y}^{(t-1)}_i - y_i$\n",
    "* $h_i = 1$\n",
    "\n",
    "Binary classification - negative binomial log-likelihood $l(y_i, \\hat{y}^{(t-1)}_i) = \\log\\left(1 + \\exp \\left(-2 y_i \\cdot \\hat{y}^{(t-1)}_i\\right)\\right)$, where $y_i \\in \\{-1,1\\}$ and $\\hat{y}^{(t-1)}_i \\in \\mathbb{R}$:\n",
    "* $g_i = - \\frac{2 y_i}{(\\exp(2 y_i \\hat{y}^{(t-1)}_i) + 1)}$\n",
    "* $h_i = \\frac{4 y_i^2 \\exp(2 y_i \\hat{y}^{(t-1)}_i))}{(\\exp(2 y_i \\hat{y}^{(t-1)}_i) + 1)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as sk_datasets\n",
    "\n",
    "import random_tree_models.decisiontree as dtree\n",
    "import random_tree_models.gradientboostedtrees as gbtree\n",
    "import random_tree_models.xgboost as xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sk_datasets.make_classification(\n",
    "    n_samples=1_000,\n",
    "    n_features=2,\n",
    "    n_classes=2,\n",
    "    n_redundant=0,\n",
    "    class_sep=2,\n",
    "    random_state=rng,\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, alpha=0.3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBoostClassifier(\n",
    "    measure_name=\"xgboost\", max_depth=2, n_trees=3, lam=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.show_tree(model.trees_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X)\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X_plot)[:, 1]\n",
    "y_prob[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_prob.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coefs = sk_datasets.make_regression(\n",
    "    n_samples=1_000, n_features=2, n_targets=1, coef=True, random_state=rng\n",
    ")\n",
    "sns.scatterplot(x=X[:, 0], y=y, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBoostRegressor(\n",
    "    measure_name=\"xgboost\", max_depth=2, n_trees=3, lam=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree.show_tree(model.trees_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_plot)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "ax = axs[0]\n",
    "sns.scatterplot(x=X_plot[:, 0], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "\n",
    "ax = axs[1]\n",
    "sns.scatterplot(x=X_plot[:, 1], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_pred.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(12, 6))\n",
    "\n",
    "ax = axs[0]\n",
    "sns.scatterplot(x=X[:, 0], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "sns.scatterplot(x=X[:, 0], y=y, ax=ax, alpha=0.1, label=\"actual\")\n",
    "\n",
    "ax = axs[1]\n",
    "sns.scatterplot(x=X[:, 1], y=y_pred, ax=ax, alpha=0.1, label=\"prediction\")\n",
    "sns.scatterplot(x=X[:, 1], y=y, ax=ax, alpha=0.1, label=\"actual\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
